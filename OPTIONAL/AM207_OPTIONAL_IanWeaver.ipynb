{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Homework (Due 11/33/2019, 11:59pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AM 207: Advanced Scientific Computing**<br>\n",
    "**Instructor: Weiwei Pan**<br>\n",
    "**Fall 2019**\n",
    "\n",
    "**Name: Ian Weaver**\n",
    "\n",
    "**Students collaborators:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "1. Implement forward mode auto-differentiation, for functions composed of 2 binary arithmetic operations (+, -) and four elementary functions (constant, power n, sin, ln).\n",
    "\n",
    "\n",
    "2. Implement reverse mode auto-differentiation (tweak or add to the code provided in Lecture #16), for functions composed of 2 binary arithmetic operations (+, -) and four elementary functions (constant, power n, sin, ln).\n",
    "\n",
    "\n",
    "3. Verify that your implementation computes derivatives correctly by comparing the derivative your implementation of auto-diff (both modes) computes versus the derivatives you compute by hand (or have Wolfram Alpha compute).\n",
    "\n",
    "\n",
    "4. Implement automatic Hessian computation by composing forward mode and reverse mode differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot configs\n",
    "fig_wide = (11, 4)\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "%matplotlib inline\n",
    "sns.set(style=\"darkgrid\", palette=\"colorblind\", color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Forward Mode Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "This is a fantastic method that can simultaneously compute a function and its \n",
    "derivative to machine precision! This can be done by introducing the dual number\n",
    "$\\epsilon^2=0$, where $\\epsilon \\ne 0$. If we transform some arbitrary function $f(x)$ \n",
    "to $f(x+\\epsilon)$ and Taylor expand it, we have: \n",
    "\n",
    "$$\n",
    "    f(x+\\epsilon) = f(x) + \\epsilon f'(x) + O\\left(\\epsilon^2\\right)\\quad.\n",
    "$$\n",
    "\n",
    "Applying the definition $\\epsilon \\equiv 0$, all second order and higher terms in\n",
    "$\\epsilon$ vanish and we are left with \n",
    "$f(x+\\epsilon) = f(x) + \\epsilon f'(x)$, where the dual part, $f'(x)$ of this transformed\n",
    "function is the derivative of our original function $f(x)$. If we adhere\n",
    "to the new system of math introduced by dual numbers, we are able to compute\n",
    "derivatives of functions exactly! \n",
    "\n",
    "For example, multiplying two dual numbers $z_1 = a_r + \\epsilon a_d$ and \n",
    "$z_2 = b_r + \\epsilon b_d$ would behave like:\n",
    "\n",
    "\\begin{align}\n",
    "    z_1 \\times z_2 &= (a_r + \\epsilon a_d) \\times (b_r + \\epsilon b_d)\n",
    "    = a_rb_r + \\epsilon(a_rb_d + a_db_r) + \\epsilon^2 a_db_d \\\\\n",
    "    &= \\boxed{a_rb_r + \\epsilon(a_rb_d + a_db_r)}\\quad.\n",
    "\\end{align}\n",
    "\n",
    "When the derivative of an elementary function is already known, we could also just do something like this:\n",
    "\n",
    "$$\n",
    "    \\ln(x) \\longrightarrow \\ln(x_r + \\epsilon x_d)\n",
    "    = \\ln(x_r) + \\epsilon \\frac{x_d}{x_r} \\quad,\n",
    "$$\n",
    "\n",
    "If we set $x_d = 1$, we automatically get the derivative of $x_r$. Operations like the ones shown above can be redefined via operator overloading. \n",
    "\n",
    "This method is also readily extended to multivariable functions with\n",
    "the introduction of **dual number basis vectors**\n",
    "$\\pmb p_i = i + \\epsilon_i 1$, where $i$ takes on any of the\n",
    "components of $\\pmb X_n$. For example, the multivariable function\n",
    "$f(x, y) = xy$ would transform like:\n",
    "\n",
    "\\begin{align}\n",
    "    \\require{cancel}\n",
    "    x \\quad\\longrightarrow\\quad& \\pmb p_x = x + \\epsilon_x + \\epsilon_y\\ 0 \\\\\n",
    "    y \\quad\\longrightarrow\\quad& \\pmb p_y = y + \\epsilon_x\\ 0 + \\epsilon_y \\\\\n",
    "    f(x, y) \\quad\\longrightarrow\\quad& f(\\pmb p) = (x + \\epsilon_x + \\epsilon_y\\ 0) \n",
    "    \\times (y + \\epsilon_x\\ 0 + \\epsilon_y) \\\\\n",
    "    &= xy + \\epsilon_y x + \\epsilon_x y + \n",
    "    \\cancel{\\epsilon_x\\epsilon_y} \\\\\n",
    "    &= xy + \\epsilon_x y + \\epsilon_y x \\quad,\n",
    "\\end{align}\n",
    "\n",
    "where we now have:\n",
    "\n",
    "\\begin{align}\n",
    "    \\newcommand{\\pd}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "    f(x+\\epsilon_x, y+\\epsilon_y) \n",
    "    &= f(x, y) + \\epsilon_x\\pd{f}{x} + \\epsilon_y\\pd{f}{y} \n",
    "    = f(x, y) + \\epsilon \\left[\\pd{f}{x},\\ \\pd{f}{y}\\right] \\\\\n",
    "    &= f(x, y) + \\epsilon \\nabla f(x, y)\\quad.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "The implementation of the overloaded multiplication operator along with `+`, `-`, `pow n`, `sin`, `ln`, are included in the `Dual` number class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from a pacakge I worked on\n",
    "# https://spacejam.readthedocs.io/en/latest/index.html\n",
    "def _toDual(obj):\n",
    "        if isinstance(obj, (int, float)):\n",
    "            return Dual(obj, 0)\n",
    "        elif isinstance(obj, Dual):\n",
    "            return obj\n",
    "        else:\n",
    "            raise Exception(f\"Type {type(obj)} not supported\")\n",
    "            \n",
    "class Dual():\n",
    "    \"\"\" Creates dual numbers and defines dual number math.\n",
    "    A real number `a` is taken in and its dual counterpart `a + eps [1.00]` is\n",
    "    returned to facilitate automatic differentiation in `ForwardDiff` .\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The dual part can optionally be returned as a \"dual basis vector\"\n",
    "    [0 1 0] if the user function `f` is multivariable and the partial\n",
    "    derivative $\\partial f / \\partial x_2$ is desired, for example.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    r : float\n",
    "        real part of `Dual` .\n",
    "    d : numpy.ndarray\n",
    "        dual part of `Dual` .\n",
    "    \"\"\"\n",
    "    def __init__(self, real, dual):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        real : int/float\n",
    "            real part of `Dual` .\n",
    "        dual : float\n",
    "            dual part of `Dual` .\n",
    "        \"\"\"\n",
    "        self.r = real\n",
    "        self.d = dual\n",
    "        self.children = []\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        \"\"\" Returns the addition of self and other\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        self: Dual object\n",
    "        other: Dual object, float, or int\n",
    "        \n",
    "        Returns\n",
    "        ------- \n",
    "        z: Dual object that is the sum of self and other\n",
    "        \"\"\"\n",
    "        other = _toDual(other)\n",
    "        real = self.r + other.r\n",
    "        dual = self.d + other.d\n",
    "\n",
    "        z = Dual(real, dual)\n",
    "        self.children.append((1.0, z))\n",
    "        other.children.append((1.0, z))\n",
    "        return z\n",
    "    \n",
    "    __radd__ = __add__ # addition commutes\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        \"\"\" Returns the subtraction of self and other\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        self: Dual object\n",
    "        other: Dual object, float, or int\n",
    "        \n",
    "        Returns\n",
    "        ------- \n",
    "        z: Dual object\n",
    "           difference of self and other\n",
    "           \n",
    "        NOTES\n",
    "        ----- \n",
    "        Subtraction does not commute in general. \n",
    "        A specialized __rsub__ is required.\n",
    "        \n",
    "        Examples\n",
    "        \"\"\"\n",
    "        other = _toDual(other)\n",
    "        real = self.r - other.r\n",
    "        dual = self.d - other.d\n",
    "        \n",
    "        z = Dual(real, dual)\n",
    "        self.children.append((1.0, z))\n",
    "        other.children.append((-1.0, z))\n",
    "        return z\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        \"\"\" Returns other - self\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        self: Dual object\n",
    "        other: Dual object, float, or int\n",
    "        \n",
    "        Returns\n",
    "        ------- \n",
    "        z: Dual object\n",
    "           difference of other and self\n",
    "        \"\"\"\n",
    "        real = other - self.r\n",
    "        dual = -self.d\n",
    "        \n",
    "        z = Dual(real, dual)\n",
    "        self.children.append((-1.0, z))\n",
    "        other.children.append((1.0, z))\n",
    "        return z\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \"\"\" Returns the product of self and other\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        self: Dual object\n",
    "        other: Dual object, float, or int\n",
    "        \n",
    "        Returns\n",
    "        ------- \n",
    "        z: Dual object that is the product of self and other\n",
    "        \"\"\"\n",
    "        other = _toDual(other)\n",
    "        real = self.r*other.r \n",
    "        dual = self.r*other.d + self.d*other.r\n",
    "        \n",
    "        z = Dual(real, dual)\n",
    "        self.children.append((other.r, z))\n",
    "        other.children.append((self.r, z))\n",
    "        return z\n",
    "\n",
    "    __rmul__ = __mul__ # multiplication commutes\n",
    "    \n",
    "    def __pow__(self, n):\n",
    "        \"\"\" Performs (self.r + eps self.d)**n\n",
    "        Parameters\n",
    "        ----------\n",
    "        self: Dual object\n",
    "        n: int or float\n",
    "        \n",
    "        Returns\n",
    "        ------- \n",
    "        z: Dual object that is self raised to the other power\n",
    "        \"\"\"\n",
    "        real = self.r**n\n",
    "        dual = self.r**(n - 1)*self.d*n\n",
    "        \n",
    "        z = Dual(real, dual)\n",
    "        self.children.append((n*self.r, z))\n",
    "        return z\n",
    "\n",
    "\n",
    "    # overload numpy functions\n",
    "    def log(self):\n",
    "        # Note: base can be changed with log(x) / log(base)\n",
    "        real = np.log(self.r)\n",
    "        dual = self.d / self.r\n",
    "        z = Dual(real, dual)\n",
    "        self.children.append((1./self.r, z))\n",
    "        return z\n",
    "\n",
    "    def sin(self):\n",
    "        real = np.sin(self.r)\n",
    "        dual = self.d*np.cos(self.r)\n",
    "        z = Dual(real, dual)\n",
    "        self.children.append((np.cos(self.r), z))\n",
    "        return z\n",
    "    \n",
    "    # format printed output\n",
    "    def __repr__(self):\n",
    "        s = f\"{self.r:} + eps {self.d}\"\n",
    "        return s\n",
    "        \n",
    "class ForwardDiff():\n",
    "    \"\"\" Performs automatic differentiation (AD) on functions input by user.\n",
    "    AD if performed by transforming `f(x1, x2, ...)` to `f(p_x1, p_x2, ...)`,\n",
    "    where `p_xi` is returned from `Dual` . \n",
    "    The final result is then returned as a `numpy.ndarray`\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    r : numpy.ndarray\n",
    "        User defined function(s) `F` evaluated at `p`.\n",
    "    d : numpy.ndarray\n",
    "        Corresponding derivative, gradient, or Jacobian of user defined\n",
    "        functions(s).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, func, p):\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        func : numpy.ndarray\n",
    "            user defined function(s).\n",
    "        p : int or numpy.ndarray\n",
    "            user defined point(s) to evaluate derivative at\n",
    "        \"\"\"\n",
    "        result = self._ad(func, p) \n",
    "        self.r = result.r\n",
    "        self.d = result.d\n",
    "\n",
    "    def _ad(self, func, p):\n",
    "        \"\"\" Internally computes `func(p)` and its derivative(s).\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        `_ad` returns a nested 1D `numpy.ndarray` to be formatted internally\n",
    "        accordingly in `ForwardDiff.__init__`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        func : numpy.ndarray\n",
    "            function(s) specified by user.\n",
    "        p : numpy.ndarray\n",
    "            point(s) specified by user.\n",
    "        \"\"\"\n",
    "        # build p_i = p + eps i\n",
    "        number_of_inputs = p.size\n",
    "        duals = np.identity(number_of_inputs)\n",
    "        ps = [Dual(p[i], duals[i, :]) for i in range(number_of_inputs)]\n",
    "\n",
    "        # perform AD with specified function\n",
    "        result = func(*ps) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Reverse Mode Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "See https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation and Lecture 17.\n",
    "\n",
    "The main takeaway is that in forward mode automatic differentiation, like in 1.1, we are doing something like this (via the chain rule):\n",
    "\n",
    "``` python\n",
    "    # computes z = x*y + sin(x) and its derivative wrt. t\n",
    "    # where,\n",
    "    # x = ?\n",
    "    # y = ?\n",
    "    # a = x*y\n",
    "    # b = sin(x)\n",
    "    # z = a + b = x*y + sin(x)\n",
    "    x  = ?\n",
    "    dxdt = ?\n",
    "\n",
    "    y  = ?\n",
    "    dydt = ?\n",
    "\n",
    "    a  = x * y\n",
    "    dadt = y * dxdt + x * dydt\n",
    "\n",
    "    b  = sin(x)\n",
    "    dbdt = cos(x) * dxdt\n",
    "\n",
    "    z  = a + b\n",
    "    dzdt = da + db\n",
    "```\n",
    "\n",
    "where the notation `dwdt` means derivative of `w` wrt. some input variable `t`. This is nice because it lends itself well to implementation using dual numbers, but it has a major disadvantage: scaling. Saw we want to calculate both `dzdx` and `dzdy` for some `x` and `y`. We would first have to seed `dxdt = 1` and `dydt = 0` run the code block above to get `dzdx`, then re-seed with `dxdt = 0` and `dydt = 1`, then re-run the block a second time to get `dzdy` . This way of computing things scales like $O(n)$ then, for $n$ inputs, which is bad for [applications](https://en.wikipedia.org/wiki/Mathematical_optimization) where $n$ can be very large.\n",
    "\n",
    "The solution is to take advantage of the symmetry of the chain rule to invert the roles of the input and output. In other words: \"instead of asking what input variables a given output variable depends on, we have to ask what output variables a given input variable can affect\". A forward and reverse pass as described in lecture will then allow us to compute the necessary gradients!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _toVar(obj):\n",
    "        if isinstance(obj, (int, float)):\n",
    "            return Var(obj)\n",
    "        elif isinstance(obj, Var):\n",
    "            return obj\n",
    "        else:\n",
    "            raise Exception(f\"Type {type(obj)} not supported\")\n",
    "            \n",
    "'''small example of reverse mode autodiff as implemented in https://github.com/Rufflewind/revad/'''\n",
    "class Var:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "        self.grad_value = None\n",
    "\n",
    "    def grad(self):\n",
    "        # propogate derivatives\n",
    "        if self.grad_value is None:\n",
    "            self.grad_value = sum(weight * var.grad()\n",
    "                                  for weight, var in self.children)\n",
    "        return self.grad_value\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = _toVar(other)\n",
    "        z = Var(self.value + other.value)\n",
    "        self.children.append((1.0, z))\n",
    "        other.children.append((1.0, z))\n",
    "        return z\n",
    "    __radd__ = __add__\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = _toVar(other)\n",
    "        z = Var(self.value - other.value)\n",
    "        self.children.append((1.0, z))\n",
    "        other.children.append((-1.0, z))\n",
    "        return z\n",
    "    __rsub__ = __sub__\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = _toVar(other)\n",
    "        z = Var(self.value * other.value)\n",
    "        self.children.append((other.value, z))\n",
    "        other.children.append((self.value, z))\n",
    "        return z\n",
    "    __rmul__ = __mul__\n",
    "\n",
    "    def __pow__(self, n):\n",
    "        z = Var(self.value**n)\n",
    "        self.children.append((n*self.value, z))\n",
    "        return z\n",
    "\n",
    "    def sin(self):\n",
    "        z = Var(np.sin(self.value))\n",
    "        self.children.append((np.cos(self.value), z))\n",
    "        return z\n",
    "\n",
    "    def log(self):\n",
    "        z = Var(np.log(self.value))\n",
    "        self.children.append((1./self.value, z))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Verify implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the arbitrary function `f` below using the elementary operations \n",
    "`+`, `-`, `pow n`, `sin`, and `log` we get the following results for the forward and reverse modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "-------\n",
      "p = [0.5 4.2]\n",
      "f(p) = 4.031964551713947\n",
      "grad[f(p)] = [8.40959306 0.28284323]\n",
      "\n",
      "reverse\n",
      "-------\n",
      "p = [0.5 4.2]\n",
      "f(p) = 4.031964551713947\n",
      "grad[f(p)] = [8.40959306 0.28284323]\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return 4 + x - np.sin(3*x*np.log(y**2)) + np.log(x**2)\n",
    "\n",
    "p = np.array([0.5, 4.2]) # location to evaluate f(p) at\n",
    "\n",
    "# forward mode \n",
    "ad = ForwardDiff(f, p)\n",
    "print(\"forward\")\n",
    "print(\"-------\")\n",
    "print(f\"p = {p}\")\n",
    "print(f\"f(p) = {ad.r}\")\n",
    "print(f\"grad[f(p)] = {ad.d}\")\n",
    "\n",
    "# reverse mode\n",
    "x = Var(p[0])\n",
    "y = Var(p[1])\n",
    "z = f(x, y)\n",
    "z.grad_value = 1 # initial seed\n",
    "print(\"\\nreverse\")\n",
    "print(\"-------\")\n",
    "grad = np.array([x.grad(), y.grad()])\n",
    "print(f\"p = {p}\")\n",
    "print(f\"f(p) = {z.value}\")\n",
    "print(f\"grad[f(p)] = {grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `p` is the starting vector that we evaluate `f(p)` at, and `grad[f(p)]` is the resulting gradient. Hopefully they match for both modes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Hessian "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to compose our forward and reverse modes to compute the Hessian matrix (H), which has the following form:\n",
    "$\\newcommand{\\pd}[1]{\\frac{\\partial f}{\\partial{#1}}}$\n",
    "$\\newcommand{\\ppd}[1]{\\frac{\\partial^2 f}{\\partial{#1}^2}}$\n",
    "$\\newcommand{\\mpd}[2]{\\frac{\\partial^2 f}{\\partial{#1}\\partial{#2}}}$\n",
    "$$\n",
    "    H = \n",
    "    \\begin{bmatrix}\n",
    "    \\ppd{x_1}      & \\cdots & \\mpd{x_1}{x_n} \\\\\n",
    "    \\vdots         & \\ddots & \\vdots \\\\\n",
    "    \\mpd{x_n}{x_1} & \\cdots & \\ppd{x_n}\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "    \\nabla f_{x_1} \\cdots \\nabla f_{x_n}\n",
    "    \\end{bmatrix}\n",
    "    \\quad,\n",
    "$$\n",
    "\n",
    "where $f_{x_i} = \\pd{x_i}$. Each column of the Hessian is a gradient, which is perfect for automatic differentiation. The reverse-on-forward method first computes the gradient vector products with the forward mode implementation. We then apply the reverse mode implementation to the result to build the Hessian. The second step needs the intermediate gradient values computed in the first step, so I added an analogous `children` field to the `Dual` class to store each node in the forward mode computation graph. Using this syntax instead of the `ForwardDiff` helper class gives me something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x children: [(1.0, 4.5 + eps [1. 0.]), (3, 1.5 + eps [3. 0.]), (1.0, 0.25 + eps [1. 0.])]\n",
      "y children: [(8.4, 17.64 + eps [0.  8.4])]\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return 4 + x - np.sin(3*x*np.log(y**2)) + np.log(x**2)\n",
    "\n",
    "p = np.array([0.5, 4.2]) # location to evaluate f(p) at\n",
    "\n",
    "\n",
    "# forward step \n",
    "x = Dual(p[0], np.array([1.0, 0.0]))\n",
    "y = Dual(p[1], np.array([0.0, 1.0]))\n",
    "z = f(x, y)\n",
    "print(f\"x children: {x.children}\")\n",
    "print(f\"y children: {y.children}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $f(p)$ and $f'(p)$ stored in the real and dual part of `z`, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.031964551713947 + eps [8.40959306 0.28284323]\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would then need to back propagate on the forward mode computation graph by composing the reverse mode with the forward mode implementation to get the second order partials that make up the Hessian. There is a very interesting discussion about it [here](https://discourse.julialang.org/t/is-there-an-efficient-way-to-compute-the-hessian-of-a-nn/26971/2) which implements this in `SparseDiffTools.jl` and [here](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#Hessian-vector-products-with-grad-of-grad) using `Jax`."
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/097ffb6a7e999bad09ad95449bea2604"
  },
  "gist": {
   "data": {
    "description": "AM207_HW7_IanWeaver.ipynb",
    "public": true
   },
   "id": "097ffb6a7e999bad09ad95449bea2604"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "505px",
    "left": "1023px",
    "top": "54px",
    "width": "185px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
