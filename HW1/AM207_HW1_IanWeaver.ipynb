{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "meECZm-xOuzJ"
   },
   "source": [
    "# Homework #1 (Due 09/18/2019, 11:59pm)\n",
    "## Maximum Likelihood Learning and Bayesian Inference\n",
    "\n",
    "**AM 207: Advanced Scientific Computing**<br>\n",
    "**Instructor: Weiwei Pan**<br>\n",
    "**Fall 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h74tcNQ2OuzK"
   },
   "source": [
    "**Name: Ian Weaver**\n",
    "\n",
    "**Students collaborators:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JvHEo6OAOuzL"
   },
   "source": [
    "### Instructions:\n",
    "\n",
    "**Submission Format:** Use this notebook as a template to complete your homework. Please intersperse text blocks (using Markdown cells) amongst `python` code and results -- format your submission for maximum readability. Your assignments will be graded for correctness as well as clarity of exposition and presentation -- a “right” answer by itself without an explanation or is presented with a difficult to follow format will receive no credit.\n",
    "\n",
    "**Code Check:** Before submitting, you must do a \"Restart and Run All\" under \"Kernel\" in the Jupyter or colab menu. Portions of your submission that contains syntactic or run-time errors will not be graded.\n",
    "\n",
    "**Libraries and packages:** Unless a problems specifically asks you to implement from scratch, you are welcomed to use any `python` library package in the standard Anaconda distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nB4Z5m9oOuzL"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output {\n",
       "    flex-direction: row;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Mount working directory\n",
    "#from google.colab import drive\n",
    "#drive.mount(\"/content/drive\")\n",
    "#%cd /content/drive/My Drive/class/am207/HW1\n",
    "\n",
    "### Import basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "### Plot configs\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "sns.set(style=\"darkgrid\", palette=\"colorblind\", color_codes=True)\n",
    "\n",
    "### Table display configs\n",
    "CSS = \"\"\"\n",
    ".output {\n",
    "    flex-direction: row;\n",
    "}\n",
    "\"\"\"\n",
    "HTML(f\"<style>{CSS}</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQhBiPF3OuzO"
   },
   "source": [
    "## Problem Description\n",
    "In the competitive rubber chicken retail market, the success of a company is built on satisfying the exacting standards of a consumer base with refined and discriminating taste. In particular, customer product reviews are all important. But how should we judge the quality of a product based on customer reviews?\n",
    "\n",
    "On Amazon, the first customer review statistic displayed for a product is the ***average rating***. The following are the main product pages for two competing rubber chicken products, manufactured by Lotus World and Toysmith respectively:\n",
    "\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- | - \n",
    "![alt](lotus1.png) |  ![alt](toysmith1.png)\n",
    "\n",
    "Clicking on the 'customer review' link on the product pages takes us to a detailed break-down of the reviews. In particular, we can now see the number of times a product is rated a given rating (between 1 and 5 stars).\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- |  - \n",
    "![alt](lotus2.png) |  ![alt](toysmith2.png)\n",
    "\n",
    "\n",
    "In the following, we will ask you to build statistical models to compare these two products using the observed rating. Larger versions of the images are available in the data set accompanying this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xY-z_KN07gAW"
   },
   "source": [
    "## Part I: A Maximum Likelihood Model\n",
    "1. **(Model Building)** Suppose that for each product, we can model the probability of the value of each new rating as the following vector:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta = [\\theta_1, \\theta_2, \\theta_3, \\theta_4, \\theta_5]\n",
    "\\end{align}\n",
    "\n",
    "  where $\\theta_i$ is the probability that a given customer will give the product $i$ number of stars. That is, each new rating (a value between 1 and 5) has a categorical distribution $Cat(\\theta)$. Represent the observed ratings of an Amazon product as a vector $R = [r_1, r_2, r_3, r_4, r_5]$ where, for example, $r_4$ is the number of $4$-star reviews out of a total of $N$ ratings. Write down the likelihood of $R$. That is, what is $p(R| \\theta)$?\n",
    "\n",
    "  **Note:** The observed ratings for each product should be read off the image files included in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODGenM8O7gAW"
   },
   "source": [
    "The Categorical distribution tells us the probabilities of each rating given by a single user. The distribution of ratings for $N$ users $(p(R|\\theta))$ represent multiple independent trials, which generalizes to the Multinomial distribution\n",
    "\n",
    "\\begin{align}\n",
    "    p(R|\\theta) \\sim \\frac{Multi(N, i | \\theta)}{N} \n",
    "    = \\boxed{\\frac{(N-1)!}{\\prod_{i=1}^5(x_i!)}\\prod_{i=1}^5 \\theta_i^{x_i}} \\quad,\n",
    "\\end{align}\n",
    "\n",
    "where $N$ is the total number of ratings given, $i$ is the rating on the 1-5 Amazon star scale, $x_i$ is the total number of ratings with score $i$, and $\\theta_i$ is the probability that a given customer will give the product a score of $i$. \n",
    "\n",
    "Note: We are dividing by $N$ because $Multi()$ gives back the absolute number of events, but we want the number $(R)$ relative to the number of trials (N).\n",
    "\n",
    "In log space this looks like\n",
    "    \n",
    "\\begin{align}\n",
    "    \\ln p(R | \\theta) = \\ln((N-1)!) - \\sum_{i=1}^5 \\ln(x_i!) + \\sum_{i=1}^5 x_i\\ln\\theta_i \\quad.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLS7eCISOuzR"
   },
   "source": [
    "2. **(Model Fitting)** Find the maximum likelihood estimator of $\\theta$ for the Lotus World model; find the MLE of $\\theta$ for the Toysmith model. You need to make a reasonably mathematical argument for why your estimate actually maximizes the likelihood (i.e. recall the criteria for a point to be a global optima of a function).\n",
    "\n",
    "  *Note:* I recommend deriving the MLE using the general expression of the likelihood. That is, derive the posterior using the variable $R$, then afterwards plug in your specific values of $R$ for each product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vD0FDurBOuzS"
   },
   "source": [
    "We can optimize this using Lagrange multipliers, which says that a function $f(x)$ for some vector of inputs $x$ with constraint $g(x) = 0$ is maximized/minimized on the stationary points of the corresponding Lagrange function $\\mathcal L(x, \\lambda) = f(x) - \\lambda(g(x))$. \n",
    "\n",
    "In our case, we want to optimimze the function $\\ln p(R|\\theta)$ given the constraint $\\sum_i\\theta_i - 1 = 0$, where $\\sum_i \\equiv \\sum_{i=1}^N$. This constraint just enforces that the probabilities of each score for a given user rating have to sum to 1. Applying this, we have\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal L(\\theta_i, \\lambda) \n",
    "    = \\ln p(R|\\theta) - \\lambda\\left(\\sum_i\\theta_i - 1\\right)\n",
    "    = \\ln((N-1)!) - \\sum_i\\ln(x_i!) + \\sum_ix_i\\ln\\theta_i - \\lambda\\sum_i\\theta_i + \\lambda\n",
    "    \\quad.\n",
    "\\end{align}\n",
    "\n",
    "Finding the stationary points next,\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{matrix}\n",
    "    \\frac{\\partial\\mathcal L}{\\partial\\theta_i}\n",
    "    = \\frac{x_i}{\\theta_i} - \\lambda = 0 \\\\\n",
    "    \\frac{\\partial\\mathcal L}{\\partial\\lambda}\n",
    "    = -\\sum_i\\theta_i + 1 = 0\n",
    "\\end{matrix}\n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\begin{matrix}\n",
    "    \\theta_i &= \\frac{x_i}{\\lambda} \\\\\n",
    "    \\sum_i\\theta_i &= 1\n",
    "\\end{matrix}\n",
    "\\end{align}\n",
    "\n",
    "Combining these two equation and solving for $\\theta_i$ gives\n",
    "\n",
    "\\begin{align}\n",
    "    \\sum_i\\frac{x_i}{\\lambda} = 1 \\quad\\Longrightarrow\\quad\n",
    "    \\lambda = \\sum_i x_i = N \\quad\\Longrightarrow\\quad\n",
    "    \\hat\\theta_i = \\frac{x_i}{N} = \\boxed{r_i} \\quad,\n",
    "\\end{align}\n",
    "\n",
    "where $\\hat\\theta_i$ is our MLE of $\\theta_i$ and $R \\equiv [r_i]$, for $i = 1,\\dots,5$.\n",
    "\n",
    "Plugging in the relative score rankings for each product, we have\n",
    "\n",
    "\\begin{align}    \n",
    "    \\boxed{\n",
    "    \\hat\\theta_\\text{Loftus} = \\left[ 0.06, 0.04, 0.06, 0.17, 0.67 \\right] \\\\\n",
    "    \\hat\\theta_\\text{Toysmith} = \\left[ 0.14, 0.08, 0.07, 0.11, 0.60 \\right]} \\quad,\n",
    "\\end{align}\n",
    "\n",
    "which is also tabulated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1568770387007,
     "user": {
      "displayName": "Ian Weaver",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAULi0maUPL32SPMz28FtLvuIKpjHw7_cntDopXLA=s64",
      "userId": "06607941177776837973"
     },
     "user_tz": 240
    },
    "id": "GtU1f7EF7gAa",
    "outputId": "e7132667-bd9a-4f10-d31a-9a80472a7212",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rating</th>\n",
       "      <th>Loftus</th>\n",
       "      <th>Toysmith</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rating  Loftus  Toysmith\n",
       "1         0.06      0.14\n",
       "2         0.04      0.08\n",
       "3         0.06      0.07\n",
       "4         0.17      0.11\n",
       "5         0.67      0.60"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints <data> in a Pandas table with rounded values for easy comparison\n",
    "def make_df(data, colnames=None):\n",
    "    df = pd.DataFrame(data, index=range(1,6), columns=colnames)\n",
    "    df.columns.name = \"rating\"\n",
    "    df = df.round(2)\n",
    "    return df\n",
    "\n",
    "# aggregate review data\n",
    "MLE_loftus = np.array([0.06, 0.04, 0.06, 0.17, 0.67], ndmin=2).T\n",
    "MLE_toysmith = np.array([0.14, 0.08, 0.07, 0.11, 0.60], ndmin=2).T\n",
    "MLE_data = np.c_[MLE_loftus, MLE_toysmith]\n",
    "\n",
    "# show the MLE for theta\n",
    "df_MLE = make_df(MLE_data, [\"Loftus\", \"Toysmith\"])\n",
    "df_MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z96YTzLBOuzT"
   },
   "source": [
    "3. **(Model Interpretation)** Based on your MLE of $\\theta$'s for both models, do you feel confident deciding if one product is superior to another? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWhlNllfOuzU"
   },
   "source": [
    "I do not have confidence one way or the other because theses MLE estimates do not have any errorbars on them. The sample size for each company is also relatively small. I would feel more confident looking at the bootstraped PI instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttnQePslOuzW"
   },
   "source": [
    "## Part II: A Bayesian Model\n",
    "\n",
    "1. **(Model Building)** Suppose you are told that customer opinions are very polarized in the retail world of rubber chickens, that is, most reviews will be 5 stars or 1 stars (with little middle ground). What would be an appropriate $\\alpha$ for the Dirichlet prior on $\\theta$? Recall that the Dirichlet pdf is given by:\n",
    "\n",
    "\\begin{align}\n",
    "p_{\\Theta}(\\theta) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}, \\quad B(\\alpha) = \\frac{\\prod_{i=1}^k\\Gamma(\\alpha_i)}{\\Gamma\\left(\\sum_{i=1}^k\\alpha_i\\right)},\n",
    "\\end{align}\n",
    "\n",
    "where $\\theta_i \\in (0, 1)$ and $\\sum_{i=1}^k \\theta_i = 1$, $\\alpha_i > 0 $ for $i = 1, \\ldots, k$.\n",
    "\n",
    "We want to describe how customers are more likely to give a 1 or 5 rating, so maybe something like \n",
    "\n",
    "\\begin{align}\n",
    "    \\alpha = [0.9, 0.1, 0.1, 0.1, 0.9] \\quad,\n",
    "\\end{align}\n",
    "\n",
    "assuming $\\alpha = [\\alpha_1, \\cdots, \\alpha_5]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxKJ4Sz7Ouzb"
   },
   "source": [
    "2. **(Inference)** Analytically derive the posterior distribution (using the likelihoods you derived in Part I) for each product.\n",
    "\n",
    "  *Note:* I recommend deriving the posterior using the general expression of a Dirichelet pdf. That is, derive the posterior using the variable $\\alpha$, then afterwards plug in your specific values of $\\alpha$ when you need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pyqtTVZIOuzb"
   },
   "source": [
    "Subbing the likelihood and prior into Bayes' theorem (and using $\\prod_i \\equiv \\prod_{i=1}^5$ for convenience),\n",
    "\n",
    "\\begin{align}\n",
    "    p(\\theta|R) &\\propto p(R|\\theta)p(\\theta) \\\\\n",
    "    &= \\left[ \\frac{(N-1)!}{\\prod_i(x_i!)}\\prod_i\\theta_i^{x_i} \\right]\n",
    "    \\frac{1}{B(\\alpha)}\\prod_i\\theta_i^{\\alpha_i - 1} \\\\\n",
    "    &= \\frac{(N-1)!}{B(\\alpha)\\prod_i(x_i!)}\\prod_i\\theta_i^{x_i + \\alpha_i - 1} \\\\\n",
    "    &\\propto Dir(x + \\alpha) \\quad.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "umXol_g47gAk"
   },
   "source": [
    "Thanks to \"our\" choice of a Dirichlet prior, our posterior is also Dirichlet because it is the conjugate prior of the Multinomial distribution! \n",
    "\n",
    "We are working in terms of absolute number of counts $(x_i)$ now instead of relative $(r_i)$, as in the MLE case. These absolute counts can be computed with $x_i = \\text{round}(r_i*N)$ since we can only have a whole number of counts for each rating. Making this conversion, we have the following posterior:\n",
    "\n",
    "\\begin{align}\n",
    "    \\boxed{p(\\theta|R) \\propto Dir \\left[ \\text{round}(N*R) + \\alpha \\right]} \\quad,\n",
    "\\end{align}\n",
    "\n",
    "where for each product we have:\n",
    "\n",
    "Company | $\\alpha$ | r | N\n",
    ":-: | :-: | :-: | :-:\n",
    "Loftus | [0.9, 0.1, 0.1, 0.1, 0.9] | [0.06, 0.04, 0.06, 0.17, 0.67] | 162\n",
    "Toysmith | [0.9, 0.1, 0.1, 0.1, 0.9] | [0.14, 0.08, 0.07, 0.11, 0.60] | 410"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ArAprs3FOuzd"
   },
   "source": [
    "3. **(The Maximum A Posterior Estimate)** Analytically or empirically compute the MAP estimate of $\\theta$ for each product, using the $\\alpha$'s you chose in Problem 1. How do these estimates compare with the MLE? Just for this problem, compute the MAP estimate of $\\theta$ for each product using a Dirichelet prior with hyperparameters $\\alpha = [1, 1, 1, 1, 1]$. Make a conjecture about the effect of the prior on the difference between the MAP estimates and the MLE's of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hc2fvJE1Ouzd"
   },
   "source": [
    "The MAP is the mode of a distribution, so let's sample from our posterior and get what that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 735,
     "status": "ok",
     "timestamp": 1568770387008,
     "user": {
      "displayName": "Ian Weaver",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAULi0maUPL32SPMz28FtLvuIKpjHw7_cntDopXLA=s64",
      "userId": "06607941177776837973"
     },
     "user_tz": 240
    },
    "id": "ZGALsMGQ7gAn",
    "outputId": "f08960d9-1b74-405b-a097-790dc1a44136"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rating</th>\n",
       "      <th>Loftus MLE</th>\n",
       "      <th>Loftus MAP</th>\n",
       "      <th>Toysmith MLE</th>\n",
       "      <th>Toysmith MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rating  Loftus MLE  Loftus MAP  Toysmith MLE  Toysmith MAP\n",
       "1             0.06        0.01          0.14          0.10\n",
       "2             0.04        0.01          0.08          0.05\n",
       "3             0.06        0.01          0.07          0.04\n",
       "4             0.17        0.08          0.11          0.07\n",
       "5             0.67        0.54          0.60          0.52"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# product review data\n",
    "N_loftus = 162\n",
    "r_loftus = np.array([0.06, 0.04, 0.06, 0.17, 0.67])\n",
    "N_toysmith = 410\n",
    "r_toysmith = np.array([0.14, 0.08, 0.07, 0.11, 0.60])\n",
    "alpha = np.array([1, 1, 1, 1, 1])\n",
    "\n",
    "# samples from modified Dirichlet dist. assuming r and alpha are numpy arrays\n",
    "# returns a (n_samples x k) array, where k = 1, ..., 5\n",
    "def get_post(r, alpha, N, n_samples):\n",
    "    x = np.array([np.round(N*r_i) for r_i in r])\n",
    "    alpha = alpha + x # avoids += int, float clash\n",
    "    return np.random.dirichlet(alpha, n_samples)\n",
    "\n",
    "# sample posterior\n",
    "post_loftus = get_post(r_loftus, alpha, N_loftus, 1_000)\n",
    "post_toysmith = get_post(r_toysmith, alpha, N_toysmith, 1_000)\n",
    "\n",
    "# compute mode of posterior for each rating\n",
    "MAP_loftus, _ = stats.mode(post_loftus, axis=0)\n",
    "MAP_toysmith, _ = stats.mode(post_toysmith, axis=0)\n",
    "MAP_loftus, MAP_toysmith = MAP_loftus.T, MAP_toysmith.T # convert to column vectors\n",
    "\n",
    "# display results\n",
    "MLE_MAP_data = np.c_[MLE_loftus, MAP_loftus, MLE_toysmith, MAP_toysmith]\n",
    "colnames = [\"Loftus MLE\", \"Loftus MAP\", \"Toysmith MLE\", \"Toysmith MAP\"]\n",
    "df_MLE_MAP = make_df(MLE_MAP_data, colnames)\n",
    "df_MLE_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From playing around with different values for $\\alpha$, it looks like the more uniform the prior, the closer the MAP is to the MLE. This makes sense because in log space the log of a uniform value is zero, leaving us with just the ln likelihood function like in the non-Bayesian case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yf_yabq8Ouzg"
   },
   "source": [
    "4. **(The Posterior Mean Estimate)** Analytically or empirically compute the posterior mean estimate of $\\theta$ for each product, using the $\\alpha$'s you chose in Problem 1. How do these estimates compare with the MAP estimates and the MLE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the mean from the sampled posterior distribution, this time computing it with $\\alpha=[0.9, 0.1, 0.1, 0.1, 0.9]$ to try and capture the polarized nature of the reviewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 730,
     "status": "ok",
     "timestamp": 1568770387010,
     "user": {
      "displayName": "Ian Weaver",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAULi0maUPL32SPMz28FtLvuIKpjHw7_cntDopXLA=s64",
      "userId": "06607941177776837973"
     },
     "user_tz": 240
    },
    "id": "eEQD127tOuzh",
    "outputId": "edf15fbf-6368-4a14-81d3-94caf3075a98",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rating</th>\n",
       "      <th>MLE Loftus</th>\n",
       "      <th>MAP Loftus</th>\n",
       "      <th>PM Loftus</th>\n",
       "      <th>MLE Toysmith</th>\n",
       "      <th>MAP Toysmith</th>\n",
       "      <th>PM Toysmith</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rating  MLE Loftus  MAP Loftus  PM Loftus  MLE Toysmith  MAP Toysmith  \\\n",
       "1             0.06        0.01       0.07          0.14          0.10   \n",
       "2             0.04        0.01       0.04          0.08          0.05   \n",
       "3             0.06        0.01       0.06          0.07          0.04   \n",
       "4             0.17        0.08       0.17          0.11          0.07   \n",
       "5             0.67        0.54       0.66          0.60          0.52   \n",
       "\n",
       "rating  PM Toysmith  \n",
       "1              0.14  \n",
       "2              0.08  \n",
       "3              0.07  \n",
       "4              0.11  \n",
       "5              0.60  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample posterior with alpha from problem 2.1\n",
    "alpha = np.array([0.9, 0.1, 0.1, 0.1, 0.9])\n",
    "post_loftus = get_post(r_loftus, alpha, N_loftus, 1_000)\n",
    "post_toysmith = get_post(r_toysmith, alpha, N_toysmith, 1_000)\n",
    "\n",
    "# compute mean of posterior for each rating\n",
    "PM_loftus = np.mean(post_loftus, axis=0, keepdims=True).T\n",
    "PM_toysmith = np.mean(post_toysmith, axis=0, keepdims=True).T\n",
    "\n",
    "# display results\n",
    "MLE_MAP_PM_data = np.c_[MLE_loftus, MAP_loftus, PM_loftus, \n",
    "                        MLE_toysmith, MAP_toysmith, PM_toysmith]\n",
    "colnames = [\"MLE Loftus\", \"MAP Loftus\", \"PM Loftus\",\n",
    "            \"MLE Toysmith\", \"MAP Toysmith\", \"PM Toysmith\"]\n",
    "make_df(MLE_MAP_PM_data, colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rF4l1CPWOuzj"
   },
   "source": [
    "5. **(The Posterior Predictive Estimate)** Sample 1000 rating vectors from the posterior predictive for each product, using the $\\alpha$'s you chose in Problem 1. Use the average of the posterior predictive samples to estimate $\\theta$. How do these estimates compare with the MAP, MLE, posterior mean estimate of $\\theta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Lecture 4, we learned that the posterior predictive can be estimated by taking our estimates for $\\theta$ from the posterior distribution and plugging them in to the Likelihood function for $R$ (i.e. the Multinomial distribution). This will give us the posterior predictive distribution, which can then be averaged to estimate the predicted values of $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 724,
     "status": "ok",
     "timestamp": 1568770387011,
     "user": {
      "displayName": "Ian Weaver",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAULi0maUPL32SPMz28FtLvuIKpjHw7_cntDopXLA=s64",
      "userId": "06607941177776837973"
     },
     "user_tz": 240
    },
    "id": "1qjed0JpOuzj",
    "outputId": "59065630-2a84-45c4-c399-2e04bb052ae4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rating</th>\n",
       "      <th>MLE Loftus</th>\n",
       "      <th>MAP Loftus</th>\n",
       "      <th>PM Loftus</th>\n",
       "      <th>PPM Loftus</th>\n",
       "      <th>MLE Toysmith</th>\n",
       "      <th>MAP Toysmith</th>\n",
       "      <th>PM Toysmith</th>\n",
       "      <th>PPM Toysmith</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rating  MLE Loftus  MAP Loftus  PM Loftus  PPM Loftus  MLE Toysmith  \\\n",
       "1             0.06        0.01       0.07        0.07          0.14   \n",
       "2             0.04        0.01       0.04        0.04          0.08   \n",
       "3             0.06        0.01       0.06        0.06          0.07   \n",
       "4             0.17        0.08       0.17        0.17          0.11   \n",
       "5             0.67        0.54       0.66        0.66          0.60   \n",
       "\n",
       "rating  MAP Toysmith  PM Toysmith  PPM Toysmith  \n",
       "1               0.10         0.14          0.14  \n",
       "2               0.05         0.08          0.08  \n",
       "3               0.04         0.07          0.07  \n",
       "4               0.07         0.11          0.11  \n",
       "5               0.52         0.60          0.60  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# allocate space to hold the posterior predictive\n",
    "N = 1_000 # number of times to sample\n",
    "post_pred_loftus, post_pred_toysmith = np.ndarray((N, 5)), np.ndarray((N, 5))\n",
    "\n",
    "# plug posterior into Likelihood to computer posterior predictive N times\n",
    "for i, (theta_loftus, theta_toysmith) in enumerate(zip(post_loftus, post_toysmith)):\n",
    "    post_pred_loftus[i] = np.random.multinomial(N_loftus, theta_loftus) / N_loftus\n",
    "    post_pred_toysmith[i] = np.random.multinomial(N_toysmith, theta_toysmith) / N_toysmith\n",
    "    \n",
    "# average together posterior predictives\n",
    "PPM_loftus = np.mean(post_pred_loftus, axis=0, keepdims=True).T\n",
    "PPM_toysmith = np.mean(post_pred_toysmith, axis=0, keepdims=True).T\n",
    "MLE_MAP_PM_PPM_data = np.c_[MLE_loftus, MAP_loftus, PM_loftus, PPM_loftus,\n",
    "                            MLE_toysmith, MAP_toysmith, PM_toysmith, PPM_toysmith]\n",
    "\n",
    "colnames = [\"MLE Loftus\", \"MAP Loftus\", \"PM Loftus\", \"PPM Loftus\",\n",
    "            \"MLE Toysmith\", \"MAP Toysmith\", \"PM Toysmith\", \"PPM Toysmith\"]\n",
    "make_df(MLE_MAP_PM_PPM_data, colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLE, MAP, posterior mean (PM), and posterior predictive mean (PPM) are all shown above\n",
    "for each company. The MLE, PM, and PPM for both products seem to be more similar to each other across all ratings than the MAP estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yM4eNwRZOuzl"
   },
   "source": [
    "6. **(Model Evaluation)** Compute the 95% credible interval of $\\theta$ for each product (*Hint: compute the 95% credible interval for each $\\theta_i$, $i=1, \\ldots, 5$*). For which product is the posterior mean and MAP estimate more reliable and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can compute the 95% CI for the PM, and PPM estimates above by using the same algorithm as in HW 1. Doing so, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 717,
     "status": "ok",
     "timestamp": 1568770387011,
     "user": {
      "displayName": "Ian Weaver",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAULi0maUPL32SPMz28FtLvuIKpjHw7_cntDopXLA=s64",
      "userId": "06607941177776837973"
     },
     "user_tz": 240
    },
    "id": "fQI48d_SOuzm",
    "outputId": "ba4d9a4b-1e37-434b-89e1-99d9dac9d76c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rating</th>\n",
       "      <th>PM Loftus</th>\n",
       "      <th>PPM Loftus</th>\n",
       "      <th>PM Toysmith</th>\n",
       "      <th>PPM Toysmith</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>$0.07^{+0.11}_{-0.03}$</td>\n",
       "      <td>$0.07^{+0.13}_{-0.02}$</td>\n",
       "      <td>$0.14^{+0.17}_{-0.11}$</td>\n",
       "      <td>$0.14^{+0.19}_{-0.09}$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>$0.04^{+0.07}_{-0.01}$</td>\n",
       "      <td>$0.04^{+0.09}_{-0.01}$</td>\n",
       "      <td>$0.08^{+0.11}_{-0.06}$</td>\n",
       "      <td>$0.08^{+0.12}_{-0.05}$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>$0.06^{+0.10}_{-0.03}$</td>\n",
       "      <td>$0.06^{+0.12}_{-0.02}$</td>\n",
       "      <td>$0.07^{+0.10}_{-0.05}$</td>\n",
       "      <td>$0.07^{+0.11}_{-0.04}$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>$0.17^{+0.23}_{-0.12}$</td>\n",
       "      <td>$0.17^{+0.26}_{-0.09}$</td>\n",
       "      <td>$0.11^{+0.14}_{-0.08}$</td>\n",
       "      <td>$0.11^{+0.15}_{-0.07}$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>$0.66^{+0.74}_{-0.59}$</td>\n",
       "      <td>$0.66^{+0.76}_{-0.56}$</td>\n",
       "      <td>$0.60^{+0.65}_{-0.55}$</td>\n",
       "      <td>$0.60^{+0.67}_{-0.53}$</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rating               PM Loftus              PPM Loftus  \\\n",
       "1       $0.07^{+0.11}_{-0.03}$  $0.07^{+0.13}_{-0.02}$   \n",
       "2       $0.04^{+0.07}_{-0.01}$  $0.04^{+0.09}_{-0.01}$   \n",
       "3       $0.06^{+0.10}_{-0.03}$  $0.06^{+0.12}_{-0.02}$   \n",
       "4       $0.17^{+0.23}_{-0.12}$  $0.17^{+0.26}_{-0.09}$   \n",
       "5       $0.66^{+0.74}_{-0.59}$  $0.66^{+0.76}_{-0.56}$   \n",
       "\n",
       "rating             PM Toysmith            PPM Toysmith  \n",
       "1       $0.14^{+0.17}_{-0.11}$  $0.14^{+0.19}_{-0.09}$  \n",
       "2       $0.08^{+0.11}_{-0.06}$  $0.08^{+0.12}_{-0.05}$  \n",
       "3       $0.07^{+0.10}_{-0.05}$  $0.07^{+0.11}_{-0.04}$  \n",
       "4       $0.11^{+0.14}_{-0.08}$  $0.11^{+0.15}_{-0.07}$  \n",
       "5       $0.60^{+0.65}_{-0.55}$  $0.60^{+0.67}_{-0.53}$  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes c% confidence interval of dist, default c = 95%\n",
    "def get_ci(dist, c=95):\n",
    "    bound = (100 - c) / 2\n",
    "    mean = np.mean(dist)\n",
    "    upper, lower = np.percentile(dist, 100 - bound), np.percentile(dist, bound)\n",
    "    return mean, upper, lower\n",
    "\n",
    "# displays CI with LaTex\n",
    "def format_ci(ci_data):\n",
    "    ci_m, ci_u, ci_d = ci_data\n",
    "    return f\"${ci_m:.2f}^{{+{ci_u:.2f}}}_{{-{ci_d:.2f}}}$\"\n",
    "\n",
    "# fill in table data\n",
    "PM_PPM_CI_data = []\n",
    "for p_loftus, pp_loftus, p_toysmith, pp_toysmith in zip(\n",
    "    post_loftus.T, post_pred_loftus.T, post_toysmith.T, post_pred_toysmith.T):\n",
    "    p_ci_loftus = get_ci(p_loftus)\n",
    "    pp_ci_loftus = get_ci(pp_loftus)\n",
    "    p_ci_toysmith = get_ci(p_toysmith)\n",
    "    pp_ci_toysmith = get_ci(pp_toysmith)\n",
    "    PM_PPM_CI_data.append([format_ci(p_ci_loftus), \n",
    "                           format_ci(pp_ci_loftus),\n",
    "                           format_ci(p_ci_toysmith),\n",
    "                           format_ci(pp_ci_toysmith)])\n",
    "    \n",
    "# display\n",
    "column_names = [\"PM Loftus\", \"PPM Loftus\", \"PM Toysmith\", \"PPM Toysmith\"]\n",
    "df = pd.DataFrame(PM_PPM_CI_data, columns=column_names, index=range(1,6))\n",
    "df.columns.name = \"rating\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the table above and the one in Problem 2.5, it looks like the PM and MAP estimates are more reliable for the Toysmith company because they are consistent with the MLE estimates for all ratings. The relative size of the uncertainties are also generally smaller for the Toysmith company, making me more confident in choosing them as the superior rubber chicken company. I think this is the case because the Toysmith company has more reviewer data points (410 vs. 162)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HENVAC4vOuzn"
   },
   "source": [
    "## Part III: Comparison\n",
    "1. **(Summarizing Customer Ratings)** Recall that on Amazon, the first customer review statistic displayed for a product is the average rating. Name at least one problem with ranking products based on the average customer rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSxN5DgSOuzo"
   },
   "source": [
    "One problem with ranking products based on average customer rating could be selection bias. Reviewers that had extreme experiences with the product (e.g. 1 or 5 star) may be more likely to leave a review than someone that just had an average experience with the product. Based on the relative number of reviews, this could skew the average towards one of those extremes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shUTEnFKOuzp"
   },
   "source": [
    "2. **(Comparison of Point Estimates)** Which point estimate (MAP, MLE, posterior mean or posterior predictive estimate) of $\\theta$, if any, would you feel choose to rank the two Amazon products? Why? \n",
    "\n",
    "  *Hint: think about which of these estimates are equivalent (if any). If they are not equivalent, what are the special properties of each estimate? What aspect of the data or the model is each estimate good at capturing?*\n",
    "  \n",
    "   **Note:** we're not looking for \"the correct answer\" here. We are looking for a sound decision based on a statistically correct interpretation of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an infinite number of reviews, the MAP should approach the MLE. I would be more confident taking the posterior mean predictive (PPM) estimate though because it is able to incorporate our prior beliefs on reviewers' bias like the PM estimate. Unlike the PM estimate though, the PPM is based on simulated fake data, which I think makes it more robust for smaller sample sizes."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AM207_HW1_IanWeaver.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "179px",
    "width": "408px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
